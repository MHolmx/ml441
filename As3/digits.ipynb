{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab041b8-4db9-402a-ae50-712b1bd40f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Callable\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import random\n",
    "import time\n",
    "from itertools import combinations\n",
    "\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860ca33-f36c-4071-ac11-e8647773d686",
   "metadata": {},
   "source": [
    "\n",
    "## Digits\n",
    "- 1,797 samples, each an 8×8 grayscale image (64 features)\n",
    "- Pixel intensities ranging from 0 to 16\n",
    "- 10 classes (digits 0–9)\n",
    "- Higher complexity. The dataset is nonlinear and requires nontrivial decision boundaries\n",
    "- Normalize pixel intensities (e.g., divide by 16). Possibly flatten into vectors of length 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa619298-8943-4a67-87ba-80c39d5143af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n",
      "Train: 1077, Val: 360, Test: 360\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data     \n",
    "y = digits.target  \n",
    "\n",
    "pd.Series(y).value_counts().sort_index().plot(\n",
    "    kind=\"bar\", color=\"skyblue\", edgecolor=\"k\", figsize=(6,4)\n",
    ")\n",
    "plt.xticks(range(10), [str(i) for i in range(10)], rotation=0)\n",
    "plt.xlabel(\"Digit Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Digits Dataset - Class Distribution\")\n",
    "plt.show()\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val   = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_dim = X_train.shape[1] \n",
    "output_dim = len(torch.unique(y_train)) \n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21b141a3-aeb4-4338-a5e0-1209874bf728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "no_classes = len(np.unique((y_train)))\n",
    "print(no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f1e9165-5d6e-40f1-8f08-9a1885594687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995ea97-e486-49b2-9c25-335d5f7c52ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab245fb-29b0-4a41-9d98-185e67509e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7c7d1d4-cbfe-4780-93ce-32b8d93822cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "SGD Done\n",
      "SCG Done\n",
      "LF Done\n",
      "Hidden size mean val losses: {256: 0.10376532179201503, 128: 0.1366665174198081, 64: 0.142345231826059, 512: 0.11989450791985301, 32: 0.18867660957786245}\n",
      "Hidden size stds: {256: 0.08026775306097715, 128: 0.10577915522108584, 64: 0.1198629668804057, 512: 0.07649486545654711, 32: 0.1500681390919455}\n",
      "Chosen hidden size (1-SE rule): 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseline_params_sgd = {\n",
    "    \"lr\": [1e-1],\n",
    "    \"momentum\": [0.9],\n",
    "    \"weight_decay\": [0]\n",
    "}\n",
    "\n",
    "baseline_params_scg = {\n",
    "    \"sigma\": [1e-6],\n",
    "    \"lambd\": [1e-4],\n",
    "    \"weight_decay\": [0]\n",
    "}\n",
    "\n",
    "baseline_params_lf = {\n",
    "    \"lr\": [1e-1],      \n",
    "    \"delta\": [1e-3],    \n",
    "    \"eps\": [1e-6],     \n",
    "    \"weight_decay\": [0]\n",
    "}\n",
    "\n",
    "hidden_grid_coarse = [32, 64, 128, 256, 512]\n",
    "print(\"Start\")\n",
    "best_cfg_stage1_sgd, results_stage1_sgd = grid_search(\n",
    "    \"SGD\", make_sgd, baseline_params_sgd, hidden_grid_coarse,\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    is_classification=True, seeds=(0,1,2)\n",
    ")\n",
    "print(\"SGD Done\")\n",
    "best_cfg_stage1_scg, results_stage1_scg = grid_search(\n",
    "    \"SCG\", make_scg, baseline_params_scg, hidden_grid_coarse,\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    is_classification=True, seeds=(0,1,2)\n",
    ")\n",
    "print(\"SCG Done\")\n",
    "best_cfg_stage1_lf, results_stage1_lf = grid_search(\n",
    "    \"LeapFrog\", make_leapfrog, baseline_params_lf, hidden_grid_coarse,\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    is_classification=True, seeds=(0,1,2)\n",
    ")\n",
    "print(\"LF Done\")\n",
    "hidden_perf_all = defaultdict(list)\n",
    "\n",
    "for results in [results_stage1_sgd, results_stage1_scg, results_stage1_lf]:\n",
    "    for r in results:\n",
    "        hidden_perf_all[r[\"hidden\"]].append(r[\"val_loss_mean\"])\n",
    "\n",
    "hidden_avg_all = {h: np.mean(scores) for h, scores in hidden_perf_all.items()}\n",
    "hidden_std_all = {h: np.std(scores) for h, scores in hidden_perf_all.items()}\n",
    "\n",
    "print(\"Hidden size mean val losses:\", hidden_avg_all)\n",
    "print(\"Hidden size stds:\", hidden_std_all)\n",
    "\n",
    "best_size = min(hidden_avg_all, key=hidden_avg_all.get)\n",
    "threshold = hidden_avg_all[best_size] + hidden_std_all[best_size]\n",
    "\n",
    "chosen_size = min([h for h, mean in hidden_avg_all.items() if mean <= threshold])\n",
    "print(\"Chosen hidden size (1-SE rule):\", chosen_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a21aa70e-8e92-40ee-9ee5-20b4302e3bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SGD config: {'hidden': 64, 'params': {'lr': 0.05, 'momentum': 0.9, 'weight_decay': 0.001}, 'val_loss_mean': 0.07176005668869173, 'val_loss_std': 0.009533140662372793, 'val_metric_mean': 0.9787037037037037, 'val_metric_std': 0.005707790743489821, 'epochs_mean': 699.4814814814815, 'epochs_std': 318.0658411189326, 'time_mean': 25.601020327320807, 'time_std': 11.869796842156644}\n",
      "Best SCG config: {'hidden': 64, 'params': {'sigma': 1e-05, 'lambd': 0.0001, 'weight_decay': 0.0001}, 'val_loss_mean': 0.035611255711949154, 'val_loss_std': 0.009932339255626352, 'val_metric_mean': 0.9888888888888889, 'val_metric_std': 0.003928371006591917, 'epochs_mean': 133.5, 'epochs_std': 16.827556764624706, 'time_mean': 5.177784899870555, 'time_std': 0.6887405499246648}\n",
      "Best LeapFrog config: {'hidden': 64, 'params': {'lr': 0.01, 'delta': 0.0001, 'eps': 1e-06, 'weight_decay': 0.001}, 'val_loss_mean': 0.30785992538159784, 'val_loss_std': 0.03270379348783965, 'val_metric_mean': 0.937037037037037, 'val_metric_std': 0.009166199015381139, 'epochs_mean': 981.0333333333333, 'epochs_std': 70.97800128678368, 'time_mean': 35.72583763864305, 'time_std': 2.710800837389019}\n"
     ]
    }
   ],
   "source": [
    "lr = [1e-3, 1e-2, 0.05, 0.1]\n",
    "momentum = [0.0, 0.9]\n",
    "weight_decay = [0, 1e-4, 1e-3]\n",
    "param_grid_sgd = {\n",
    "    \"lr\": lr,\n",
    "    \"momentum\": momentum,\n",
    "    \"weight_decay\": weight_decay\n",
    "}\n",
    "\n",
    "param_grid_scg = {\n",
    "    \"sigma\": [1e-5, 1e-6, 1e-7],\n",
    "    \"lambd\": [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "    \"weight_decay\": [0, 1e-4, 1e-3]\n",
    "}\n",
    "\n",
    "lr = [1e-3, 1e-2, 1e-1]\n",
    "delta = [1e-4, 1e-3, 1e-2]\n",
    "eps = [1e-6, 3e-6, 1e-5]\n",
    "weight_decay = [0, 1e-4, 1e-3]\n",
    "param_grid_lf = {\n",
    "    \"lr\": lr,\n",
    "    \"delta\": delta,\n",
    "    \"eps\": eps,\n",
    "    \"weight_decay\": weight_decay\n",
    "}\n",
    "\n",
    "best_cfg_sgd, results_sgd = grid_search(\n",
    "    \"SGD\", make_sgd, param_grid_sgd, [chosen_size],\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    is_classification=True, seeds=(0,1,2)\n",
    ")\n",
    "\n",
    "best_cfg_scg, results_scg = grid_search(\n",
    "    \"SCG\", make_scg, param_grid_scg, [chosen_size],\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    is_classification=True, seeds=(0,1,2)\n",
    ")\n",
    "\n",
    "best_cfg_lf, results_lf = grid_search(\n",
    "    \"LeapFrog\", make_leapfrog, param_grid_lf, [chosen_size],\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    is_classification=True, seeds=(0,1,2)\n",
    ")\n",
    "\n",
    "print(\"Best SGD config:\", best_cfg_sgd)\n",
    "print(\"Best SCG config:\", best_cfg_scg)\n",
    "print(\"Best LeapFrog config:\", best_cfg_lf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fe31b71-7a2f-46ec-aa03-6ff55043de60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SGD Results ===\n",
      "Test accuracy: 0.978 ± 0.005\n",
      "Test F1-score (macro): 0.977 ± 0.005\n",
      "Test loss: 0.0808 ± 0.0113\n",
      "Runtime: 11.605 ± 2.614 sec\n",
      "Epochs to early-stop: 269.2 ± 53.8\n",
      "\n",
      "=== SCG Results ===\n",
      "Test accuracy: 0.974 ± 0.005\n",
      "Test F1-score (macro): 0.973 ± 0.006\n",
      "Test loss: 0.1687 ± 0.1281\n",
      "Runtime: 19.771 ± 8.383 sec\n",
      "Epochs to early-stop: 287.9 ± 120.2\n",
      "\n",
      "=== LeapFrog Results ===\n",
      "Test accuracy: 0.942 ± 0.007\n",
      "Test F1-score (macro): 0.941 ± 0.007\n",
      "Test loss: 0.4632 ± 0.2073\n",
      "Runtime: 10.933 ± 2.583 sec\n",
      "Epochs to early-stop: 244.8 ± 57.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_sgd, test_accs_sgd, test_losses_sgd = evaluate_optimizer(\"SGD\", make_sgd, best_cfg_sgd, chosen_size, n_runs=10)\n",
    "results_scg, test_accs_scg, test_losses_scg = evaluate_optimizer(\"SCG\", make_scg, best_cfg_scg, chosen_size, n_runs=10)\n",
    "results_lf, test_accs_lf, test_losses_lf  = evaluate_optimizer(\"LeapFrog\", make_leapfrog, best_cfg_lf, chosen_size, n_runs=10)\n",
    "\n",
    "\n",
    "for res in [results_sgd, results_scg, results_lf]:\n",
    "    print(f\"\\n=== {res['optimizer']} Results ===\")\n",
    "    print(f\"Test accuracy: {res['mean_acc']:.3f} ± {res['std_acc']:.3f}\")\n",
    "    print(f\"Test F1-score (macro): {res['mean_f1']:.3f} ± {res['std_f1']:.3f}\")\n",
    "    print(f\"Test loss: {res['mean_loss']:.4f} ± {res['std_loss']:.4f}\")\n",
    "    print(f\"Runtime: {res['mean_runtime']:.3f} ± {res['std_runtime']:.3f} sec\")\n",
    "    print(f\"Epochs to early-stop: {res['mean_epochs']:.1f} ± {res['std_epochs']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35b3fe8a-9a3e-43f2-b7f8-8884de90f0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA across optimizers: F=101.4845, p=2.76e-13\n",
      "\n",
      "Tukey HSD results:\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "==================================================\n",
      "group1 group2 meandiff p-adj  lower  upper  reject\n",
      "--------------------------------------------------\n",
      "    LF    SCG   0.0319    0.0 0.0251 0.0388   True\n",
      "    LF    SGD   0.0358    0.0  0.029 0.0427   True\n",
      "   SCG    SGD   0.0039 0.3506 -0.003 0.0107  False\n",
      "--------------------------------------------------\n",
      "\n",
      "SGD vs SCG:\n",
      "  t-test: t=1.606, p=0.1260\n",
      "  Mann-Whitney U: U=72.000, p=0.0975\n",
      "\n",
      "SGD vs LF:\n",
      "  t-test: t=12.563, p=0.0000\n",
      "  Mann-Whitney U: U=100.000, p=0.0002\n",
      "\n",
      "SCG vs LF:\n",
      "  t-test: t=10.734, p=0.0000\n",
      "  Mann-Whitney U: U=100.000, p=0.0002\n"
     ]
    }
   ],
   "source": [
    "F, p = stats.f_oneway(test_accs_sgd, test_accs_scg, test_accs_lf)\n",
    "print(f\"ANOVA across optimizers: F={F:.4f}, p={p:.4g}\")\n",
    "\n",
    "all_accs = np.concatenate([\n",
    "    test_accs_sgd,\n",
    "    test_accs_scg,\n",
    "    test_accs_lf\n",
    "])\n",
    "labels = (\n",
    "    [\"SGD\"] * len(test_accs_sgd) +\n",
    "    [\"SCG\"] * len(test_accs_scg) +\n",
    "    [\"LF\"]  * len(test_accs_lf)\n",
    ")\n",
    "\n",
    "tukey = pairwise_tukeyhsd(all_accs, labels, alpha=0.05)\n",
    "print(\"\\nTukey HSD results:\")\n",
    "print(tukey)\n",
    "\n",
    "pairs = [\n",
    "    (\"SGD\", test_accs_sgd, \"SCG\", test_accs_scg),\n",
    "    (\"SGD\", test_accs_sgd, \"LF\",  test_accs_lf),\n",
    "    (\"SCG\", test_accs_scg, \"LF\",  test_accs_lf)\n",
    "]\n",
    "\n",
    "for a, x, b, y in pairs:\n",
    "    t, p_t = stats.ttest_ind(x, y, equal_var=False)  # Welch’s t-test\n",
    "    U, p_u = stats.mannwhitneyu(x, y, alternative=\"two-sided\")\n",
    "    print(f\"\\n{a} vs {b}:\")\n",
    "    print(f\"  t-test: t={t:.3f}, p={p_t:.4f}\")\n",
    "    print(f\"  Mann-Whitney U: U={U:.3f}, p={p_u:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0a2d672-fb7e-4a9f-802a-d1507c23fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA across optimizers: F=18.2009, p=9.886e-06\n",
      "\n",
      "Tukey HSD results:\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      "group1 group2 meandiff p-adj   lower   upper  reject\n",
      "----------------------------------------------------\n",
      "    LF    SCG  -0.2945 0.0004 -0.4591 -0.1299   True\n",
      "    LF    SGD  -0.3824    0.0  -0.547 -0.2178   True\n",
      "   SCG    SGD  -0.0879 0.3944 -0.2525  0.0767  False\n",
      "----------------------------------------------------\n",
      "\n",
      "SGD vs SCG:\n",
      "  t-test: t=-2.051, p=0.0701\n",
      "  Mann-Whitney U: U=21.000, p=0.0312\n",
      "\n",
      "SGD vs LF:\n",
      "  t-test: t=-5.526, p=0.0004\n",
      "  Mann-Whitney U: U=0.000, p=0.0002\n",
      "\n",
      "SCG vs LF:\n",
      "  t-test: t=-3.626, p=0.0025\n",
      "  Mann-Whitney U: U=9.000, p=0.0022\n"
     ]
    }
   ],
   "source": [
    "F, p = stats.f_oneway(test_losses_sgd, test_losses_scg, test_losses_lf)\n",
    "print(f\"ANOVA across optimizers: F={F:.4f}, p={p:.4g}\")\n",
    "\n",
    "# --- 2. Tukey HSD (pairwise post-hoc) ---\n",
    "all_losses = np.concatenate([\n",
    "    test_losses_sgd,\n",
    "    test_losses_scg,\n",
    "    test_losses_lf\n",
    "])\n",
    "labels = (\n",
    "    [\"SGD\"] * len(test_losses_sgd) +\n",
    "    [\"SCG\"] * len(test_losses_scg) +\n",
    "    [\"LF\"]  * len(test_losses_lf)\n",
    ")\n",
    "\n",
    "tukey = pairwise_tukeyhsd(all_losses, labels, alpha=0.05)\n",
    "print(\"\\nTukey HSD results:\")\n",
    "print(tukey)\n",
    "\n",
    "# --- 3. Pairwise t-tests and Mann–Whitney U ---\n",
    "pairs = [\n",
    "    (\"SGD\", test_losses_sgd, \"SCG\", test_losses_scg),\n",
    "    (\"SGD\", test_losses_sgd, \"LF\",  test_losses_lf),\n",
    "    (\"SCG\", test_losses_scg, \"LF\",  test_losses_lf)\n",
    "]\n",
    "\n",
    "for a, x, b, y in pairs:\n",
    "    t, p_t = stats.ttest_ind(x, y, equal_var=False)  # Welch’s t-test\n",
    "    U, p_u = stats.mannwhitneyu(x, y, alternative=\"two-sided\")\n",
    "    print(f\"\\n{a} vs {b}:\")\n",
    "    print(f\"  t-test: t={t:.3f}, p={p_t:.4f}\")\n",
    "    print(f\"  Mann-Whitney U: U={U:.3f}, p={p_u:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f63f96-ee20-46fc-98fc-b351fccd52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "def make_sgd(params, lr=1e-2, momentum=0.9, weight_decay=1e-4):\n",
    "    import torch.optim as optim\n",
    "    return optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "def make_scg(params, sigma=1e-4, lambd=1e-3, weight_decay=0.0):\n",
    "    return SCG(params, sigma=sigma, lambd=lambd, weight_decay=weight_decay)\n",
    "\n",
    "def make_leapfrog(params, lr=1e-2, weight_decay=0.0, delta=1e-3, eps=1e-6):\n",
    "    return LeapFrog(\n",
    "        params,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        delta=delta,\n",
    "        eps=eps\n",
    "    )\n",
    "\n",
    "def _flatten_params(params):\n",
    "    vec = []\n",
    "    for p in params:\n",
    "        if p.requires_grad:\n",
    "            vec.append(p.view(-1))  \n",
    "    return torch.cat(vec)\n",
    "\n",
    "def _flatten_grads(params):\n",
    "    vec = []\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            vec.append(p.grad.view(-1))\n",
    "        else:\n",
    "            vec.append(torch.zeros_like(p.data).view(-1))\n",
    "    return torch.cat(vec)\n",
    "\n",
    "def _assign_flat_params(params, flat):\n",
    "    idx = 0\n",
    "    for p in params:\n",
    "        n = p.numel()\n",
    "        p.data.copy_(flat[idx:idx+n].view_as(p))\n",
    "\n",
    "        idx += n\n",
    "\n",
    "def _flat(params):\n",
    "    return torch.cat([p.view(-1) for p in params])\n",
    "\n",
    "def _flat_grads(params):\n",
    "    vec = []\n",
    "    for p in params:\n",
    "        if p.grad is None:\n",
    "            vec.append(torch.zeros_like(p).view(-1))\n",
    "        else:\n",
    "            vec.append(p.grad.view(-1))\n",
    "    return torch.cat(vec)\n",
    "\n",
    "def _assign(params, flat):\n",
    "    i = 0\n",
    "    for p in params:\n",
    "        n = p.numel()\n",
    "        p.data.copy_(flat[i:i+n].view_as(p))\n",
    "        i += n\n",
    "\n",
    "class SCG(Optimizer):\n",
    "\n",
    "    def __init__(self, params, sigma=1e-4, lambd=1e-3, weight_decay=0.0,\n",
    "                 restart_every=None, tol_grad=1e-6):\n",
    "        super().__init__(params, dict(sigma=sigma, lambd=lambd,\n",
    "                                       weight_decay=weight_decay,\n",
    "                                       restart_every=restart_every,\n",
    "                                       tol_grad=tol_grad))\n",
    "        self._state = None  \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure):\n",
    "\n",
    "        group = self.param_groups[0]\n",
    "        sigma0 = group['sigma']\n",
    "        wd     = group['weight_decay']\n",
    "        tol_g  = group['tol_grad']\n",
    "        Nrst   = group['restart_every']\n",
    "\n",
    "        params = [p for p in group['params'] if p.requires_grad]\n",
    "\n",
    "        # ---- evaluate E(w), ∇E(w)\n",
    "        with torch.enable_grad():\n",
    "            loss = closure()\n",
    "        for p in params:\n",
    "            if p.grad is not None: p.grad.zero_()\n",
    "        loss.backward()\n",
    "\n",
    "        w = _flat(params)\n",
    "        g = _flat_grads(params)              \n",
    "        if wd != 0.0: g = g + wd * w\n",
    "        r = -g                                  \n",
    "\n",
    "        # ---- init state\n",
    "        st = self._state\n",
    "        if st is None:\n",
    "            st = dict(w=w.clone(), p=r.clone(), r=r.clone(),\n",
    "                      lambd=group['lambd'], lambd_bar=0.0,\n",
    "                      success=True, it=0)\n",
    "            self._state = st\n",
    "\n",
    "        p         = st['p']\n",
    "        r_prev    = st['r']\n",
    "        lambd     = st['lambd']\n",
    "        lambd_bar = st['lambd_bar']\n",
    "        success   = st['success']\n",
    "        it        = st['it']\n",
    "\n",
    "        if torch.norm(g).item() < tol_g:\n",
    "            return loss\n",
    "\n",
    "        if success:\n",
    "            p_norm = torch.norm(p).item() + 1e-12\n",
    "            sigma_k = sigma0 / p_norm\n",
    "            w_sigma = w + sigma_k * p\n",
    "            _assign(params, w_sigma)\n",
    "\n",
    "            with torch.enable_grad():\n",
    "                loss_sigma = closure()\n",
    "            for q in params:\n",
    "                if q.grad is not None: q.grad.zero_()\n",
    "            loss_sigma.backward()\n",
    "\n",
    "            g_sigma = _flat_grads(params)\n",
    "            if wd != 0.0: g_sigma = g_sigma + wd * w_sigma\n",
    "            s = (g_sigma - g) / sigma_k         \n",
    "            delta = torch.dot(p, s).item()       \n",
    "\n",
    "            _assign(params, w)\n",
    "        else:\n",
    "            p_norm = torch.norm(p).item() + 1e-12\n",
    "            delta = 0.0\n",
    "\n",
    "        p2 = p_norm * p_norm\n",
    "        delta = delta + (lambd - lambd_bar) * p2\n",
    "\n",
    "        if delta <= 0.0:\n",
    "            lambd_bar = 2.0 * (lambd - delta / p2)\n",
    "            delta     = -delta + lambd * p2\n",
    "            lambd     = lambd_bar\n",
    "\n",
    "        mu   = torch.dot(p, r).item()\n",
    "        alpha = mu / (delta + 1e-12)\n",
    "\n",
    "        w_new = w + alpha * p\n",
    "        _assign(params, w_new)\n",
    "        with torch.enable_grad():\n",
    "            loss_new = closure()\n",
    "        for q in params:\n",
    "            if q.grad is not None: q.grad.zero_()\n",
    "        loss_new.backward()\n",
    "        g_new = _flat_grads(params)\n",
    "        if wd != 0.0: g_new = g_new + wd * w_new\n",
    "        r_new = -g_new\n",
    "\n",
    "        Delta = 2.0 * delta * (loss.item() - loss_new.item()) / (mu*mu + 1e-12)\n",
    "\n",
    "        if Delta >= 0.0:\n",
    "            # 7) accept\n",
    "            success   = True\n",
    "            lambd_bar = 0.0\n",
    "            # conjugacy\n",
    "            it_next = it + 1\n",
    "            if (Nrst is not None) and (it_next % Nrst == 0):\n",
    "                beta = 0.0\n",
    "            else:\n",
    "                beta = max(0.0, (torch.dot(r_new, r_new).item() - torch.dot(r_new, r_prev).item()) / (mu + 1e-12))\n",
    "            p_next = r_new + beta * p\n",
    "\n",
    "            if Delta >= 0.75:\n",
    "                lambd = 0.25 * lambd\n",
    "\n",
    "            # commit new state\n",
    "            self._state.update(dict(w=w_new.clone(), p=p_next.clone(), r=r_new.clone(),\n",
    "                                    lambd=lambd, lambd_bar=lambd_bar, success=success, it=it_next))\n",
    "            return loss_new\n",
    "        else:\n",
    "            success   = False\n",
    "            lambd_bar = lambd\n",
    "            if Delta < 0.25:\n",
    "                lambd = lambd + delta * (1.0 - Delta) / (p2 + 1e-12)\n",
    "            _assign(params, w)\n",
    "            self._state.update(dict(w=w.clone(), p=p.clone(), r=r.clone(),\n",
    "                                    lambd=lambd, lambd_bar=lambd_bar, success=success, it=it))\n",
    "            return loss\n",
    "\n",
    "class LeapFrog(Optimizer):\n",
    "    def __init__(self, params, lr=1e-2, momentum=0.9, weight_decay=0.0,\n",
    "                 delta=1e-3, eps=1e-6, max_steps=1000):\n",
    "        defaults = dict(lr=lr, momentum=momentum,\n",
    "                        weight_decay=weight_decay,\n",
    "                        delta=delta, eps=eps,\n",
    "                        max_steps=max_steps)\n",
    "        super().__init__(params, defaults)\n",
    "        self.state = {\"k\": 0, \"i\": 0, \"j\": 2, \"s\": 0, \"p\": 1}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure):\n",
    "\n",
    "\n",
    "        group = self.param_groups[0]\n",
    "        lr = group['lr']\n",
    "        momentum = group['momentum']\n",
    "        wd = group['weight_decay']\n",
    "        delta = group['delta']\n",
    "        eps = group['eps']\n",
    "\n",
    "        params = [p for p in group['params'] if p.requires_grad]\n",
    "\n",
    "        # helper flatteners\n",
    "        def flat_params():\n",
    "            return torch.cat([p.view(-1) for p in params])\n",
    "\n",
    "        def flat_grads():\n",
    "            return torch.cat([\n",
    "                (torch.zeros_like(p).view(-1) if p.grad is None else p.grad.view(-1))\n",
    "                for p in params\n",
    "            ])\n",
    "\n",
    "        def assign_flat(vec):\n",
    "            idx = 0\n",
    "            for p in params:\n",
    "                n = p.numel()\n",
    "                p.data.copy_(vec[idx:idx+n].view_as(p))\n",
    "                idx += n\n",
    "\n",
    "        loss = closure()\n",
    "        for p in params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "        loss.backward()\n",
    "\n",
    "        w = flat_params()\n",
    "        g = flat_grads()\n",
    "        if wd != 0.0:\n",
    "            g = g + wd * w\n",
    "\n",
    "        st = self.state\n",
    "        k = st[\"k\"]\n",
    "\n",
    "        if k == 0:\n",
    "            a0 = -g\n",
    "            v0 = 0.5 * a0\n",
    "            st.update({\"a\": a0, \"v\": v0, \"w\": w.clone()})\n",
    "            st[\"k\"] = 1\n",
    "            return loss\n",
    "\n",
    "        a = -g\n",
    "        v = st[\"v\"] + a * lr\n",
    "        w_new = w + v * lr\n",
    "\n",
    "        assign_flat(w_new)\n",
    "\n",
    "        st.update({\"a\": a.clone(), \"v\": v.clone(), \"w\": w_new.clone(), \"k\": k+1})\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def train_one_model(\n",
    "    model, optimizer, criterion, train_loader, val_loader,\n",
    "    epochs=200, patience=25, device=\"cpu\", is_classification=True,\n",
    "    use_closure=False\n",
    "):\n",
    "    start_time = time.time()\n",
    "    model.to(device)\n",
    "    best_state, best_val = None, np.inf if not is_classification else -np.inf\n",
    "    wait = 0\n",
    "\n",
    "    hist = {\"train_loss\": [], \"val_loss\": [], \"val_metric\": []}\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        total_loss, total_correct, total_n = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "                if is_classification:\n",
    "                    pred = out.argmax(dim=1)\n",
    "                    total_correct += (pred == yb).sum().item()\n",
    "                total_n += xb.size(0)\n",
    "        val_loss = total_loss / total_n\n",
    "        if is_classification:\n",
    "            val_metric = total_correct / total_n \n",
    "        else:\n",
    "            val_metric = -val_loss  \n",
    "        return val_loss, val_metric\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            if use_closure:\n",
    "                def closure():\n",
    "                    with torch.enable_grad():  \n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                        out = model(xb)\n",
    "                        loss = criterion(out, yb)\n",
    "                    return loss\n",
    "                loss = optimizer.step(closure)\n",
    "                epoch_loss += loss.item() * xb.size(0)\n",
    "            else:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            n += xb.size(0)\n",
    "\n",
    "        train_loss, train_metric = evaluate(train_loader)\n",
    "        val_loss, val_metric = evaluate(val_loader)\n",
    "        hist[\"train_loss\"].append(train_loss)\n",
    "        hist[\"val_loss\"].append(val_loss)\n",
    "        hist[\"val_metric\"].append(val_metric)\n",
    "        \n",
    "        score = val_metric if is_classification else -val_loss\n",
    "        if score > (best_val if is_classification else -best_val):\n",
    "            best_val = score\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "                \n",
    "    runtime = time.time() - start_time\n",
    "    epochs_trained = len(hist[\"train_loss\"])\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, hist, epochs_trained, runtime\n",
    "\n",
    "\n",
    "def _f1_score(preds: torch.Tensor,\n",
    "              targets: torch.Tensor,\n",
    "              num_classes: int = None,\n",
    "              average: str = \"weighted\") -> float:\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = int(max(preds.max(), targets.max()).item() + 1)\n",
    "\n",
    "    cm = torch.bincount(\n",
    "        (targets * num_classes + preds).to(torch.long),\n",
    "        minlength=num_classes * num_classes\n",
    "    ).reshape(num_classes, num_classes)\n",
    "\n",
    "    TP = cm.diag()\n",
    "    FP = cm.sum(dim=0) - TP\n",
    "    FN = cm.sum(dim=1) - TP\n",
    "    # Per-class F1\n",
    "    denom = (2 * TP + FP + FN).clamp(min=1)   \n",
    "    f1_per_class = (2 * TP) / denom\n",
    "\n",
    "    if average == \"macro\":\n",
    "        present = cm.sum(dim=1) > 0\n",
    "        if present.any():\n",
    "            return f1_per_class[present].float().mean().item()\n",
    "        return 0.0\n",
    "    elif average == \"weighted\":\n",
    "        support = cm.sum(dim=1).float()\n",
    "        total = support.sum().clamp(min=1.0)\n",
    "        return (f1_per_class.float() * support / total).sum().item()\n",
    "    elif average == \"micro\":\n",
    "        TP_sum = TP.sum().float()\n",
    "        FP_sum = FP.sum().float()\n",
    "        FN_sum = FN.sum().float()\n",
    "        denom = (2 * TP_sum + FP_sum + FN_sum).clamp(min=1.0)\n",
    "        return (2 * TP_sum / denom).item()\n",
    "\n",
    "def evaluate_test(model, criterion, X_test, y_test,\n",
    "                  is_classification: bool = True,\n",
    "                  device: str = \"cpu\",\n",
    "                  f1_average: str = \"weighted\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        outputs = model(X_test)\n",
    "        loss = criterion(outputs, y_test)\n",
    "\n",
    "        if is_classification:\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            acc = (preds == y_test).float().mean().item()\n",
    "            f1 = _f1_score(preds.cpu(), y_test.cpu(), average=f1_average)\n",
    "            return loss.item(), acc, f1\n",
    "        else:\n",
    "            mse = nn.MSELoss()(outputs, y_test).item()\n",
    "            mae = nn.L1Loss()(outputs, y_test).item()\n",
    "            return mse, mae\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def grid_search(\n",
    "    alg_name, alg_ctor, param_grid, hidden_grid,\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    is_classification=True, device=\"cpu\",\n",
    "    seeds=(0,1,2,3,4)\n",
    "):\n",
    "    results = []\n",
    "    times = []\n",
    "    epochs_list = []\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(torch.unique(y_train)) if is_classification else 1\n",
    "    criterion = nn.CrossEntropyLoss() if is_classification else nn.MSELoss()\n",
    "\n",
    "    for h in hidden_grid:\n",
    "        for params in product(*param_grid.values()):\n",
    "            kwargs = dict(zip(param_grid.keys(), params))\n",
    "            scores = []\n",
    "            losses = []\n",
    "\n",
    "            for seed in seeds:\n",
    "                set_seed(seed)\n",
    "            \n",
    "                model = NeuralNet(input_dim, h, output_dim)\n",
    "                optimizer = alg_ctor(model.parameters(), **kwargs)\n",
    "                use_closure = (alg_name in {\"SCG\",\"LeapFrog\"})\n",
    "            \n",
    "\n",
    "                train_loader = DataLoader(TensorDataset(X_train, y_train),\n",
    "                                          batch_size=len(X_train), shuffle=True)\n",
    "                val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "            \n",
    "                model, hist, epochs_trained, runtime = train_one_model(\n",
    "                    model, optimizer, criterion,\n",
    "                    train_loader, val_loader,\n",
    "                    epochs=1000, patience=100,\n",
    "                    device=device,\n",
    "                    is_classification=is_classification,\n",
    "                    use_closure=use_closure\n",
    "                )\n",
    "            \n",
    "                if is_classification:\n",
    "                    best_loss = min(hist[\"val_loss\"])\n",
    "                    losses.append(best_loss)\n",
    "                    scores.append(max(hist[\"val_metric\"]))\n",
    "                else:\n",
    "                    best_loss = min(hist[\"val_loss\"])\n",
    "                    losses.append(best_loss)\n",
    "            \n",
    "                epochs_list.append(epochs_trained)\n",
    "                times.append(runtime)\n",
    "            \n",
    "            mean_loss = np.mean(losses)\n",
    "            std_loss  = np.std(losses)\n",
    "            mean_score = np.mean(scores) if scores else None\n",
    "            std_score  = np.std(scores) if scores else None\n",
    "            mean_epochs = np.mean(epochs_list)\n",
    "            std_epochs  = np.std(epochs_list)\n",
    "            mean_time = np.mean(times)\n",
    "            std_time  = np.std(times)\n",
    "            \n",
    "            results.append({\n",
    "                \"hidden\": h,\n",
    "                \"params\": kwargs,\n",
    "                \"val_loss_mean\": mean_loss,\n",
    "                \"val_loss_std\": std_loss,\n",
    "                \"val_metric_mean\": mean_score,\n",
    "                \"val_metric_std\": std_score,\n",
    "                \"epochs_mean\": mean_epochs,\n",
    "                \"epochs_std\": std_epochs,\n",
    "                \"time_mean\": mean_time,\n",
    "                \"time_std\": std_time\n",
    "            })\n",
    "\n",
    "    results_sorted = sorted(\n",
    "        results,\n",
    "        key=lambda r: r[\"val_loss_mean\"],\n",
    "        reverse=False\n",
    "    )\n",
    "    best_cfg = results_sorted[0]\n",
    "\n",
    "    return best_cfg, results_sorted\n",
    "\n",
    "def paired_sign_test(x, y):\n",
    "    d = np.array(x) - np.array(y)\n",
    "    d = d[d != 0]\n",
    "    n = len(d)\n",
    "    if n == 0:\n",
    "        return 1.0  \n",
    "    k = int(np.sum(d > 0))  \n",
    "\n",
    "    def binom_cdf(k, n):\n",
    "        return sum(math.comb(n,i) for i in range(0, k+1)) / (2**n)\n",
    "    cdf = binom_cdf(k, n)\n",
    "    sf  = 1 - binom_cdf(k-1, n) \n",
    "    p = 2 * min(cdf, sf)\n",
    "    return min(1.0, p)\n",
    "\n",
    "def bootstrap_mean_ci(deltas, B=20000, alpha=0.05, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    deltas = np.array(deltas)\n",
    "    means = []\n",
    "    n = len(deltas)\n",
    "    for _ in range(B):\n",
    "        sample = deltas[rng.integers(0, n, size=n)]\n",
    "        means.append(sample.mean())\n",
    "    lo = np.percentile(means, 100*alpha/2)\n",
    "    hi = np.percentile(means, 100*(1 - alpha/2))\n",
    "    return float(np.mean(deltas)), float(lo), float(hi)\n",
    "\n",
    "def compare_hidden_sizes(results_dict):\n",
    "\n",
    "    data = [(h, s) for h, scores in results_dict.items() for s in scores]\n",
    "    df = pd.DataFrame(data, columns=[\"hidden\", \"val_acc\"])\n",
    "\n",
    "    groups = [df[df.hidden == h][\"val_acc\"].values for h in sorted(df.hidden.unique())]\n",
    "\n",
    "    if len(groups) > 2:\n",
    "        f_stat, p_val = stats.f_oneway(*groups)\n",
    "        print(f\"\\nANOVA across {len(groups)} groups: F={f_stat:.4f}, p={p_val:.4g}\")\n",
    "    else:\n",
    "        print(\"\\nANOVA skipped (need ≥3 groups).\")\n",
    "\n",
    "    if len(groups) > 2:\n",
    "        tukey = pairwise_tukeyhsd(endog=df[\"val_acc\"], groups=df[\"hidden\"], alpha=0.05)\n",
    "        print(\"\\nTukey HSD results:\\n\", tukey)\n",
    "\n",
    "    print(\"\\nPairwise tests:\")\n",
    "    for h1, h2 in combinations(sorted(results_dict.keys()), 2):\n",
    "        x, y = results_dict[h1], results_dict[h2]\n",
    "\n",
    "        t_stat, t_p = stats.ttest_ind(x, y, equal_var=False)\n",
    "\n",
    "        u_stat, u_p = stats.mannwhitneyu(x, y, alternative=\"two-sided\")\n",
    "\n",
    "        print(f\"{h1} vs {h2}: t={t_stat:.3f}, p={t_p:.4g} | U={u_stat:.3f}, p={u_p:.4g}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_optimizer(optimizer_name, make_opt_fn, best_cfg, chosen_size,\n",
    "                       n_runs=30, seed_start=10):\n",
    "    \n",
    "    test_accs, test_f1s, test_losses = [], [], []\n",
    "    runtimes, avg_epochs = [], []\n",
    "    \n",
    "    for seed in range(seed_start, seed_start + n_runs):\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # Model\n",
    "        model = NeuralNet(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim=chosen_size,\n",
    "            output_dim=no_classes\n",
    "        )\n",
    "\n",
    "     \n",
    "        optimizer = make_opt_fn(\n",
    "            model.parameters(),\n",
    "            **best_cfg[\"params\"]   \n",
    "        )\n",
    "        \n",
    "     \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        use_closure_flag = True if (optimizer_name == \"SCG\" or optimizer_name == \"LeapFrog\") else False\n",
    "\n",
    "        model, hist, epochs_trained, runtime = train_one_model(\n",
    "            model, optimizer, criterion,\n",
    "            train_loader, val_loader,\n",
    "            epochs=1000, patience=100,\n",
    "            device=\"cpu\",\n",
    "            is_classification=True,\n",
    "            use_closure=use_closure_flag\n",
    "        )\n",
    "\n",
    "        \n",
    "       \n",
    "        test_loss, test_acc, test_f1 = evaluate_test(\n",
    "            model, criterion, X_test, y_test,\n",
    "            is_classification=True, device=\"cpu\", f1_average=\"macro\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        test_accs.append(test_acc)\n",
    "        test_f1s.append(test_f1)\n",
    "        test_losses.append(test_loss)\n",
    "        runtimes.append(runtime)\n",
    "        avg_epochs.append(epochs_trained)\n",
    "\n",
    "    results = {\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"mean_acc\": np.mean(test_accs), \"std_acc\": np.std(test_accs),\n",
    "        \"mean_f1\": np.mean(test_f1s), \"std_f1\": np.std(test_f1s),\n",
    "        \"mean_loss\": np.mean(test_losses), \"std_loss\": np.std(test_losses),\n",
    "        \"mean_runtime\": np.mean(runtimes), \"std_runtime\": np.std(runtimes),\n",
    "        \"mean_epochs\": np.mean(avg_epochs), \"std_epochs\": np.std(avg_epochs),\n",
    "    }\n",
    "    \n",
    "    return results, test_accs, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb493e1d-8982-4bae-bf64-272c85a87695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
